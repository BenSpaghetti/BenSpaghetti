---
layout: post
title: Calculus III - Lecture 6
date: 2024-02-22
categories: maths
tags: calculus_iii
math: true
---

Lecture highlights:

1. symmetries
2. quadrics
2. high school mathematics
2. multivariable calculus
2. high school mathematics

## Meth

Obviously he like these stuff, that's why he mething a lot these past two lectures. 

Physics books rarely give precise definitions. You just grow used to them. Mathematics is different.

Symmetry is something you should always keep in mind. My personal research is about symmetry, geometry, and physics. The things I think about always have three meanings, but the most important one is symmetry. It gives rise to geometry and physics. What we see, what is really behind them is symmetry. You cannot appreciate it without sufficient mathematical training. You will realise that symmetry is the thing behind mathematics. Well, interesting matheamtics, the core of mathemation. Normal people see a plane and just see a plane.Trained mathematicans see translational and rotational symmetry. [insert physics stuff that I don't know]. 

(Everything he start mething I start furiously typing lol. Dk what he things of that.)

I emphasise that symmetry is important because when I teach calculus and geometry, when I solve the problems, I always try to observe the symmetry. When I solve it, I always try to preserve the symmetry. It makes things easier. For example, in multivariable calculus, which I taught last term,  when you do surface integrals, you should respect the symmetry when choosing your coordinate systems. Some problems that are very messy in rectangular coordinate systems become easy in spherical coordinates.

We have a so-called mathematical value system. We were so brainwashed. Sometimes we believe what is good maths? It means solving very hard problems. This is totally wrong. These maths competitions, some people think if you are good at them then you are good at maths. No. In maths we try to understand higher and higher stuff. Mathematical competitions treat you like a machine. They don't train you to think. They train you to compute. Humans can do something  machines cannot. Namely, we can think and we can judge. This problem (referring to quadrics, or really conics), what is the three dimensional version?

In maths, (I noted down new maths for some reason but idk why this is new), you can do many things like this. You can publish many many papers. I don't want to write that many papers. I write papers because I want to help some young people. 

We are so used to using variables, but in nature, there is no such thing called variables. There is no coordinate systems. We introduce them for convenience, just like this whiteboard eraser I'm holding. We introduce algebra to help us do things in symmetries and physics. Algebra is a tool, that's why it is not as interesting. There are things beyond intuition, where we can just do computation with algebra. These can be done by machine. Machines are also tools. 

Many mathematicians only use bottom index. Thats bad. Because using top index also can give you more information. This is common practice in theoretical physics, but mathematicians seem to reject it. In mathematics, part of the thing we do is invent notation. If we have good notation, people can learn things faster. Newton had the dots notation for derivatives. Leibniz has his notation. Leibniz's notation won over time, because it is better. The formulas are easier to remember.

If you are not sophisticated, you don't like generalisations, because it takes effort. Most people don't like this. They want free stuff. But somethings if you just do a little bit of extra work, you reach the essence. Then this saves effort. In problems, including high school problems, you can ask what is a family of problems. The sneaky part is that they often just give the case when $n=3$ or something as the problem. If they asked you to solve it in the general case, then you would know that there must be a simpler method.

## Lecture Notes

Well I mean this whole thing is lecture notes, but I rarely see people put meth in math lecture notes. 

I came in late this lecture. The board just said examples, I guess they are examples of automorphisms/symmetries, which are structure-preserving transformations. 

1. For any set $X$ which contains objects, we have the set of automorphisms of $X$, the set $\mathrm{Aut} X$. This is isomorphic to the symmetric, also called permutation, group $S_n$, for $\|X\|$ a positive integer. 
2. For a real linear space of dimension $n$ $V$, $\mathrm{Aut} V = \mathrm{GL}(V) \cong \mathrm{GL}_n(\mathbb{R})$. Notice(!) that we are saying $\mathrm{GL}$ is isomorphic to $\mathrm{GL}_n$ because the former is abstract linear transformation while the latter is a set of matrices.
3. For an Euclidean space $V$ of dimension $n$, $\mathrm{Aut}V$ is more often known as $\mathrm{O}(V)\cong \mathrm{O}(n)$. 
4. For an real affine space $\mathbb{A}$ of dimension $n$, $\mathrm{Aut}(\mathbb{A}) = \mathrm{Aff}(\mathbb{A}) \cong \mathrm{Aff}(\mathbb{A}_n^\mathbb{R})= \mathbb{R}^n \rtimes \mathrm{GL}_n(\mathbb{R})$. 

$\rtimes$ is the (right) [semidirect product](https://en.wikipedia.org/wiki/Semidirect_product). Recall that an affine mapping is something like $\vec{x} \mapsto A\vec{x} + \vec{b}$. The right semidirect product is one where there is a [right split exact sequence](https://en.wikipedia.org/wiki/Exact_sequence) as follows:

<!-- https://q.uiver.app/#q=WzAsMTEsWzAsMSwiMSJdLFsxLDEsIlxcbWF0aGJie1J9Xm4iXSxbMiwxLCJcXG1hdGhybXtBZmZ9KFxcbWF0aGJie0F9X1xcbWF0aGJie1J9Xm4pIl0sWzMsMSwiXFxtYXRocm17R0x9X24oXFxtYXRoYmJ7Un0pIl0sWzQsMSwiMSJdLFsyLDAsIihBLCBcXHZlY3swfSkiXSxbMywwLCJBIl0sWzEsMiwiXFx2ZWN7Yn0iXSxbMiwyLCIoSSwgXFx2ZWN7Yn0pIl0sWzIsMywiKEEsIFxcdmVje2J9KSJdLFszLDMsIkEiXSxbMCwxXSxbMSwyLCIiLDAseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6InRvcCJ9fX1dLFsyLDMsIiIsMCx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6ImVwaSJ9fX1dLFszLDRdLFs2LDUsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XSxbNyw4LCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9LCJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtYXBzIHRvIn19fV0sWzksMTAsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XV0= -->
<iframe class="quiver-embed" src="https://q.uiver.app/#q=WzAsMTEsWzAsMSwiMSJdLFsxLDEsIlxcbWF0aGJie1J9Xm4iXSxbMiwxLCJcXG1hdGhybXtBZmZ9KFxcbWF0aGJie0F9X1xcbWF0aGJie1J9Xm4pIl0sWzMsMSwiXFxtYXRocm17R0x9X24oXFxtYXRoYmJ7Un0pIl0sWzQsMSwiMSJdLFsyLDAsIihBLCBcXHZlY3swfSkiXSxbMywwLCJBIl0sWzEsMiwiXFx2ZWN7Yn0iXSxbMiwyLCIoSSwgXFx2ZWN7Yn0pIl0sWzIsMywiKEEsIFxcdmVje2J9KSJdLFszLDMsIkEiXSxbMCwxXSxbMSwyLCIiLDAseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6InRvcCJ9fX1dLFsyLDMsIiIsMCx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6ImVwaSJ9fX1dLFszLDRdLFs2LDUsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XSxbNyw4LCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9LCJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtYXBzIHRvIn19fV0sWzksMTAsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XV0=&embed" width="776" height="560" style="border-radius: 8px; border: none;"></iframe>

<!-- % https://q.uiver.app/#q=WzAsMTEsWzAsMSwiMSJdLFsxLDEsIlxcbWF0aGJie1J9Xm4iXSxbMiwxLCJcXG1hdGhybXtBZmZ9KFxcbWF0aGJie0F9X1xcbWF0aGJie1J9Xm4pIl0sWzMsMSwiXFxtYXRocm17R0x9X24oXFxtYXRoYmJ7Un0pIl0sWzQsMSwiMSJdLFsyLDAsIihBLCBcXHZlY3swfSkiXSxbMywwLCJBIl0sWzEsMiwiXFx2ZWN7Yn0iXSxbMiwyLCIoSSwgXFx2ZWN7Yn0pIl0sWzIsMywiKEEsIFxcdmVje2J9KSJdLFszLDMsIkEiXSxbMCwxXSxbMSwyLCIiLDAseyJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJob29rIiwic2lkZSI6InRvcCJ9fX1dLFsyLDMsIiIsMCx7InN0eWxlIjp7ImhlYWQiOnsibmFtZSI6ImVwaSJ9fX1dLFszLDRdLFs2LDUsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XSxbNyw4LCIiLDAseyJzaG9ydGVuIjp7InNvdXJjZSI6MjAsInRhcmdldCI6MjB9LCJzdHlsZSI6eyJ0YWlsIjp7Im5hbWUiOiJtYXBzIHRvIn19fV0sWzksMTAsIiIsMCx7InNob3J0ZW4iOnsic291cmNlIjoyMCwidGFyZ2V0IjoyMH0sInN0eWxlIjp7InRhaWwiOnsibmFtZSI6Im1hcHMgdG8ifX19XV0=
\[\begin{tikzcd}
	&& {(A, \vec{0})} & A \\
	1 & {\mathbb{R}^n} & {\mathrm{Aff}(\mathbb{A}_\mathbb{R}^n)} & {\mathrm{GL}_n(\mathbb{R})} & 1 \\
	& {\vec{b}} & {(I, \vec{b})} \\
	&& {(A, \vec{b})} & A
	\arrow[from=2-1, to=2-2]
	\arrow[hook, from=2-2, to=2-3]
	\arrow[two heads, from=2-3, to=2-4]
	\arrow[from=2-4, to=2-5]
	\arrow[shorten <=5pt, shorten >=5pt, maps to, from=1-4, to=1-3]
	\arrow[shorten <=4pt, shorten >=4pt, maps to, from=3-2, to=3-3]
	\arrow[shorten <=5pt, shorten >=5pt, maps to, from=4-3, to=4-4]
\end{tikzcd}\] -->

5. For an Euclidean space $\mathbb{E}$ of dimension $n$ (which is isomorphic to $\mathbb{E}^n$), we have $\mathrm{Aut}(\mathbb{E}) \cong \mathrm{Aut}(\mathbb{E}^m) = \mathbf{E} \equiv \mathbb{R}^n \rtimes \mathrm{O}(n)$, which $\equiv$ once again means natural identification. Note again that this is stronger than mere isomorphism. $\mathbf{E}$ is the group of $n$-dimensional Euclidean symmetrices. Below is suppose to be an exact sequence for this but I don't get why only four points. Maybe I copy wrong

<!-- https://q.uiver.app/#q=WzAsNCxbMCwwLCIxIl0sWzEsMCwiXFxtYXRoYmJ7Un1ebiJdLFsyLDAsIlxcbWF0aHJte099KG4pIl0sWzMsMCwiMSJdLFswLDFdLFsxLDJdLFsyLDNdLFsyLDEsIiIsMSx7ImN1cnZlIjoyfV1d -->
<iframe class="quiver-embed" src="https://q.uiver.app/#q=WzAsNCxbMCwwLCIxIl0sWzEsMCwiXFxtYXRoYmJ7Un1ebiJdLFsyLDAsIlxcbWF0aHJte099KG4pIl0sWzMsMCwiMSJdLFswLDFdLFsxLDJdLFsyLDNdLFsyLDEsIiIsMSx7ImN1cnZlIjoyfV1d&embed" width="564" height="176" style="border-radius: 8px; border: none;"></iframe>

<!-- % https://q.uiver.app/#q=WzAsNCxbMCwwLCIxIl0sWzEsMCwiXFxtYXRoYmJ7Un1ebiJdLFsyLDAsIlxcbWF0aHJte099KG4pIl0sWzMsMCwiMSJdLFswLDFdLFsxLDJdLFsyLDNdLFsyLDEsIiIsMSx7ImN1cnZlIjoyfV1d
\[\begin{tikzcd}
	1 & {\mathbb{R}^n} & {\mathrm{O}(n)} & 1
	\arrow[from=1-1, to=1-2]
	\arrow[from=1-2, to=1-3]
	\arrow[from=1-3, to=1-4]
	\arrow[curve={height=12pt}, from=1-3, to=1-2]
\end{tikzcd}\] -->

There is also a group of special Euclidean symmetries $\mathbf{SE}_n \equiv \mathbb{R}^n\rtimes \mathrm{SO}(n)$, which preserves orientation due to the determinant. This represents the rigid motions of $\mathbb{E}^n$. It is also called the $n$-th special Euclidean group. 

Note that many of M's terminology are from Wikipedia. 

Now we consider a classification of quadrics ($\mathbb{R}^2$ for now, so conics), which is an example in his book. When we classify, we want to classify things up to canonical forms. In his example in the book, he did it up to rigid motion. When we classify quadrics in $\mathbb{R}^3$, we often do it up to affine equivalence to simplify our life. This includes scaling. 

The theory of affine spaces has an application to high school geometry. He proved the midpoint theorem for ten minutes.

We move onto multivariable calculus. In this course, we consider smooth functions. For an open set $U \in T(\mathbb{E}^n)$, consider $f: U\to \mathbb{R}$. Suppose $p\in U \ni (x^1, \dots, x^n)$. Then $f$ is smooth (infinitely differentiable, $C^\infty$) on $U$ if $\frac{\partial^n f}{(\partial x^i)^n}$ exists and is continuous on $U$. Here most of the readers will know something. So I will just say new stuff.

This may sound weird, but remember that we are still working in Euclidean spaces, and not on smooth manifolds, so some of this stuff probably seems unnecessary. We consider the tangent space of $U$ at $p \in U$, which we denote by $T_p U$. It is defined as the tangent space $T_p \mathbb{E}^n$ of $\mathbb{E}^n$ at $p$, which is defined as $\{\overrightarrow{pq}\mid q \in \mathbb{E}^n\}\equiv \{(p,\vec{u})\mid \vec{u} \in \mathbb{R}^n\} = \{p\}\times\mathbb{R}^n \equiv \mathbb{R}^n$. So it has a vector space structure. As in, $\alpha(p, \vec{u}) + \beta(p, \vec{w}) = (p, \alpha\vec{u} + \beta\vec{w})$. An intrinsic formulation of partial differentiation is as below. 

$$
\frac{\partial f}{\partial x^i} (p) := \lim_{t \to 0} \frac{f(p+ t\vec{e_i}) - f(p)}{t} = \left. \frac{d}{dt}\right|_{t=0} f(p + t\vec{e_i}). 
$$

This is the first level of generalisation of partial derivative. For the second level, we consider an arbitrary normal unit vector $\vec{n}$:

$$
\frac{\partial f}{\partial \vec{n}} (p) = \left. \frac{d}{dt}\right|_{t=0} f(p + t\vec{n}).
$$

This is the directional derivative of $f$ at $p$ along $\vec{n}$.

At the end, we consider a high school mathematics problem.